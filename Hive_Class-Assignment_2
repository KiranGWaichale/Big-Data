Scenario Based questions:

Scenario: Will the reducer work or not if you use “Limit 1” in any HiveQL query?
---> Reducer is not used when tried to select data from sales_data_orc table. So Reducer is not usedwhen we use limit.


Scenario: Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time?
---> Maybe they can use but the HQL operations like delete, truncate, drop will impact more.


Scenario: Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
---> As we want to calculate the total revenue generated for each month, so we can use partitioning. Partitionong is dividing data into table based on a specific column.
To partition do not use Primary Key or unique valued columns, prefer to do partition on column value with less uniqueness.
2 Types of Partitioning:
1. Static
2. Dynamic
Both have there own Pros and Cons, Dyanic is prefered when we don't now the ditinct values or have large dataset and Static is prefered when have small dataset.

To implement dynamic partition we need to set below property, which allows full scan of data from A to Z.
set hive.exec.dynamic.partition.mode=nonstrict;

To solve the above issue,
1. Prefer to use Dynmic partitiong as we have a lasrge dataset
2. create a new table with partitioned by month
3. overwrite the date from base table
4. Dynamic parttion will create 12 partition which will contain data of specific month
5. Then you can query on individual table and extract the total revenue for each month.



Scenario: How can you add a new partition for the month December in the above partitioned table?
---> As we just want to add only on partition for month December so we can use static partition. HQL query as follows,
# HQL Query to create table
create table transaction_details_december (cust_id int, amount float, country string) partitioned by (month string);

# HQL query to insert data of December month
insert overwrite table transaction_details_december partition(month) select cust_id, amount, country, month from transaction_details;



Scenario: I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
---> set the below property which allows full scanning of data in dynamic partitioning.
set hive.exec.dynamic.partition.mode=nonstrict;



Scenario: Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
---> 



Scenario: Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
--->



Scenario: LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;
The following statement failed to execute. What can be the cause
---> incomplete path, 'file://' is not there, '/' which indicates root not there and filename is also not there.
To execute the above query and load data into table path should be like follows,
load data local inpath 'file:///Home/country/state/address_info.csv' overwrite into table address;




Scenario: Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
---> 
